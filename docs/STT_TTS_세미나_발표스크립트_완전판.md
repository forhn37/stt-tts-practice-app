# 🎙️ STT/TTS 세미나 발표 스크립트 (완전판)

> **총 시간: 3시간**
> - 이론: 2시간
> - 실습: 1시간
> 
> **청중**: 개발자, PM, 기획자, 팀장, 프리랜서 등 혼합
> 
> 📌 스크립트 원칙:
> - 비개발자도 따라올 수 있게 쉬운 설명
> - 개발자를 위한 심화 포인트 별도 표시 [🔧 개발자 포인트]
> - 실습은 체험 위주, 코드는 참고용으로만

---

# 📋 전체 타임테이블

| 시간 | 구분 | 내용 |
|------|------|------|
| 0:00-0:05 | 오프닝 | 인사, 일정 안내 |
| 0:05-0:30 | **Chapter 1** | 소리의 디지털화 (이론) |
| 0:30-1:00 | **Chapter 2** | STT 심화 (이론) |
| 1:00-1:10 | ☕ 휴식 | 10분 |
| 1:10-1:35 | **Chapter 3** | TTS 심화 (이론) |
| 1:35-1:55 | **Chapter 4** | 평가지표와 실무 (이론) |
| 1:55-2:00 | 전환 | 실습 환경 안내 |
| 2:00-2:15 | **실습 1** | Wave Lab - 소리 시각화 |
| 2:15-2:35 | **실습 2** | STT Lab - 음성 인식 체험 |
| 2:35-2:50 | **실습 3** | TTS Lab - 음성 합성 체험 |
| 2:50-3:00 | 마무리 | Q&A, 클로징 |

---

# 🎬 오프닝 (5분)

## 슬라이드 1: 타이틀

**[시각자료: 타이틀 슬라이드]**

> 안녕하세요, 여러분. 오늘 '소리와 언어의 연결: STT & TTS 원리 탐구'라는 주제로 세미나를 진행하게 된 [강사명]입니다.
>
> 오늘 참석하신 분들 중에 개발자분들도 계시고, 기획자분들, PM분들, 팀장님들도 계신 걸로 알고 있어요. 다양한 배경을 가진 분들이 모이셨네요.
>
> 그래서 오늘은 **"코드를 몰라도 이해할 수 있게, 하지만 개발자분들도 실무에 바로 쓸 수 있게"** 두 가지를 모두 잡아보려고 합니다.
>
> AI 스피커한테 말 걸어보신 적 있으시죠? "오늘 날씨 어때?" 하면 대답하잖아요. 그게 어떻게 가능한지, 오늘 그 비밀을 파헤쳐볼 거예요.

---

## 슬라이드 2: 일정 안내

**[시각자료: 타임테이블]**

> 오늘 일정 말씀드릴게요. 총 3시간입니다.
>
> 앞에 2시간은 **이론**이에요. 소리가 어떻게 텍스트가 되고, 텍스트가 어떻게 소리가 되는지 원리를 배웁니다.
>
> 뒤에 1시간은 **실습**이에요. 제가 준비한 웹 앱으로 직접 체험해보실 거예요. 마이크로 녹음하고, 파형도 보고, 음성 인식도 해보고요.
>
> 중간에 10분 쉬는 시간 있고요, 마지막에 Q&A 시간도 있습니다.
>
> 아, 실습하실 때 **Chrome이나 Edge 브라우저** 필요해요. 혹시 Firefox나 Safari 쓰시는 분은 미리 Chrome 준비해주세요. 이따 실습 시간에 다시 말씀드릴게요.
>
> 자, 그럼 시작해볼까요?

---

# 📘 Chapter 1: 컴퓨터는 소리를 어떻게 이해하는가? (25분)

## 슬라이드 3: 챕터 1 타이틀

**[시각자료: Chapter 1 타이틀]**

> 첫 번째 챕터, **"컴퓨터는 소리를 어떻게 이해하는가?"**입니다.
>
> 핵심 질문 하나 던져볼게요.
>
> 우리가 말을 하면 공기가 떨리잖아요. 연속적인 진동이에요. 근데 컴퓨터는 0과 1만 알아요. **이 둘을 어떻게 연결할까요?**
>
> 이 질문에 답하는 게 STT, TTS 모두의 기초입니다.

---

## 슬라이드 4: 소리의 3요소

**[시각자료: 파형 다이어그램]**

> 먼저 소리가 뭔지부터 볼게요.
>
> 소리는 **공기의 진동**이에요. 물리적으로는 압력파라고 해요.
>
> 소리에는 세 가지 특성이 있어요.
>
> **첫째, 진폭** - 파동의 높이예요. 쉽게 말하면 **볼륨**이죠. 크게 말하면 진폭이 커져요.
>
> **둘째, 주파수** - 1초에 몇 번 떨리느냐예요. **음의 높낮이**를 결정해요. 여자 목소리가 남자보다 높은 건 주파수가 높아서예요.
>
> **셋째, 파형** - 떨리는 모양이에요. 같은 '도' 음이라도 피아노랑 기타 소리가 다르죠? 그게 **음색**이고, 파형의 차이예요.

---

## 슬라이드 5: 아날로그 vs 디지털

**[시각자료: 아날로그/디지털 비교]**

> 소리는 **아날로그**예요. 끊김 없이 쭉 이어지는 연속적인 곡선이죠.
>
> 근데 컴퓨터는 **디지털**이에요. 0과 1, 이산적인 값만 처리할 수 있어요.
>
> 그래서 **ADC**, Analog-to-Digital Conversion이 필요해요. 아날로그를 디지털로 바꾸는 거죠.
>
> 왜 굳이 디지털로 바꾸냐? 장점이 있거든요.
> - 저장하고 전송하기 쉬워요
> - 복사해도 품질이 안 떨어져요 (카세트테이프 생각해보세요, 복사하면 음질 떨어졌잖아요)
> - 컴퓨터로 분석하고 가공할 수 있어요

---

## 슬라이드 6: 샘플링과 양자화

**[시각자료: ADC 다이어그램]**

> ADC에서 중요한 개념 두 가지!
>
> **첫째, 샘플링(Sampling)** - "얼마나 자주 측정할까?"
>
> **둘째, 양자화(Quantization)** - "얼마나 정밀하게 기록할까?"
>
> 하나씩 볼게요.

---

## 슬라이드 7: 샘플링 레이트

**[시각자료: 01_sampling_rate_comparison.png]**

> **샘플링 레이트**는 1초에 몇 번 소리를 측정하느냐예요. 단위는 Hz(헤르츠).
>
> 이 그림 보세요. 맨 위가 원본이에요. 연속적인 곡선.
>
> 중간은 촘촘하게 샘플링한 거예요. 점이 많죠? 원본을 잘 따라가요.
>
> 맨 아래는 듬성듬성 샘플링한 거예요. 점이 적어서 원본이랑 많이 달라졌죠. **정보가 손실**된 거예요.
>
> 실제로 어디에 어떤 샘플링을 쓰냐면요,
> - 전화 통화: 8,000Hz (좀 뭉개지죠?)
> - **음성 인식: 16,000Hz** (이게 표준이에요)
> - CD 음질: 44,100Hz (음악 감상용)

---

## 슬라이드 8: 나이퀴스트 정리

**[시각자료: 나이퀴스트 공식]**

> 여기서 중요한 이론! **나이퀴스트 정리**.
>
> **"원본을 복원하려면 최소 2배 이상으로 샘플링해야 한다."**
>
> 예를 들어 CD가 44,100Hz잖아요? 그러면 최대 22,050Hz까지 담을 수 있어요.
>
> 사람 귀가 듣는 범위가 대략 20~20,000Hz거든요. 그래서 CD면 사람이 듣는 모든 소리를 커버해요.
>
> 그럼 **음성 인식에서 왜 16kHz를 쓸까요?**
>
> 사람 목소리의 핵심 주파수가 300~3,400Hz 정도예요. 16,000 ÷ 2 = 8,000Hz까지 커버하니까 충분해요. 굳이 더 높게 할 필요가 없는 거죠. 데이터도 아끼고요.
>
> [🔧 개발자 포인트] API 쓸 때 오디오 포맷 확인하세요. 44.1kHz WAV 파일을 그대로 보내면 불필요하게 데이터가 커요. 16kHz로 리샘플링하면 용량 줄고 속도도 빨라져요.

---

## 슬라이드 9: 비트 깊이

**[시각자료: 비트 깊이 설명]**

> 두 번째 개념, **비트 깊이**.
>
> 각 샘플의 크기를 몇 단계로 표현하느냐예요.
>
> 예를 들어 원본 값이 0.7342... 이렇게 소수점 쭉 있다고 해요.
>
> **8비트**면 256단계밖에 없어요. 0.73 정도로 대충 반올림해야 해요. 오차가 커요.
>
> **16비트**면 65,536단계예요. 0.7342로 정밀하게 표현할 수 있어요. 거의 원본이랑 같아요.
>
> 그래서 **STT/TTS 표준이 16kHz, 16비트**예요.
>
> 참고로 1초 오디오가 약 32KB, 1분이면 1.9MB 정도 됩니다.

---

## 슬라이드 10: 시간 도메인 vs 주파수 도메인

**[시각자료: 02_time_vs_frequency_domain.png]**

> 자, 이제 소리를 보는 **두 가지 관점**!
>
> 왼쪽 그래프, **시간 도메인**. X축이 시간, Y축이 진폭이에요. "이 순간 소리가 얼마나 크냐?"를 보는 거죠. 우리가 흔히 보는 파형이에요.
>
> 근데 이걸로는 **"무슨 음이 섞여있는지"** 알기 어려워요.
>
> 오른쪽 그래프, **주파수 도메인**. X축이 주파수, Y축이 세기예요. "어떤 높이의 소리가 얼마나 강하게 있느냐?"를 보는 거죠.
>
> 이 변환을 하는 게 **푸리에 변환**이에요.
>
> 근데 실제로는 **STFT**라고, 시간을 짧게 잘라서 각각 푸리에 변환을 해요. 그러면 **"시간에 따라 주파수가 어떻게 변하는지"** 볼 수 있거든요.

---

## 슬라이드 11: 스펙트로그램

**[시각자료: 03_spectrogram_example.png]**

> STFT 결과를 그림으로 그린 게 **스펙트로그램**이에요.
>
> X축이 시간, Y축이 주파수, 색깔이 에너지(세기)예요. 밝을수록 그 시점에 그 주파수가 강하다는 뜻이에요.
>
> 보시면 음성이 있는 구간과 조용한 구간이 확실히 다르죠?
>
> **STT 모델은 바로 이걸 입력으로 받아요.** 스펙트로그램 이미지를 보고 "이게 무슨 말이지?" 맞추는 거예요.

---

## 슬라이드 12: Mel-Spectrogram

**[시각자료: 04_mel_filterbank.png]**

> 근데 스펙트로그램을 그대로 쓰기보다 한 단계 더 가공해요.
>
> **Mel 스케일**을 적용해요. 사람 귀의 특성을 반영한 거예요.
>
> 그림 보시면, 저주파 쪽에는 필터가 촘촘하고, 고주파 쪽에는 듬성듬성해요.
>
> 왜 이렇게 하냐? **사람 귀가 저주파 변화에는 민감하고, 고주파 변화에는 둔감하거든요.**
>
> 이렇게 만든 게 **Mel-Spectrogram**. 거의 모든 STT/TTS 모델이 이걸 씁니다.

---

## 슬라이드 13: Chapter 1 정리

**[시각자료: 요약 박스]**

> 챕터 1 정리할게요.
>
> 1. 소리는 아날로그 → 디지털로 변환 필요
> 2. 핵심은 **샘플링 레이트**(얼마나 자주)와 **비트 깊이**(얼마나 정밀하게)
> 3. STT/TTS 표준: **16kHz, 16비트**
> 4. 시간 도메인 → **주파수 도메인**(STFT)
> 5. 최종 입력 데이터: **Mel-Spectrogram**
>
> 여기까지 질문? (잠시 대기)
>
> 좋습니다. 다음 챕터로 넘어갈게요.

---

# 📘 Chapter 2: STT - 소리를 텍스트로 (30분)

## 슬라이드 14: 챕터 2 타이틀

**[시각자료: Chapter 2 타이틀]**

> 두 번째 챕터, **STT(Speech-to-Text)** 심화예요.
>
> 챕터 1에서 소리를 Mel-Spectrogram으로 바꾸는 건 배웠죠?
>
> 근데 이 스펙트로그램에서 어떻게 "안녕하세요"라는 글자가 나올까요?
>
> 생각보다 어려운 문제예요.

---

## 슬라이드 15: Alignment 문제

**[시각자료: Alignment 다이어그램]**

> STT의 핵심 난제, **Alignment(정렬) 문제**!
>
> "안녕"이라고 말한다고 해요. 어떤 사람은 빠르게 0.3초 만에, 어떤 사람은 천천히 1초 동안 말해요.
>
> 스펙트로그램 프레임 수가 다 다르죠? 근데 결과는 똑같이 "안녕"이 나와야 해요.
>
> 그럼 **몇 번째 프레임이 "안"이고 몇 번째가 "녕"인지** 어떻게 알아요? 사람이 일일이 표시해야 하나요?
>
> 그건 너무 힘들잖아요. 그래서 **자동으로 정렬을 학습하는 방법**이 나왔어요.

---

## 슬라이드 16: CTC 개념

**[시각자료: CTC 설명]**

> 그게 바로 **CTC(Connectionist Temporal Classification)**예요. 2006년에 나왔어요.
>
> CTC의 핵심 아이디어:
>
> **"프레임마다 어떤 글자인지 정확히 몰라도, 최종 텍스트만 맞으면 학습할 수 있다!"**
>
> 어떻게요? 두 가지 트릭이 있어요.
>
> 1. **Blank 토큰(ε)** 도입
> 2. **중복 제거 규칙**

---

## 슬라이드 17: Blank 토큰

**[시각자료: Blank 토큰 설명]**

> **Blank(ε)**는 "아무것도 출력하지 않음"이에요. 진짜 공백 문자가 아니에요!
>
> 왜 필요하냐?
>
> **첫째**, 프레임 수와 글자 수 차이를 해결해요. 프레임이 100개인데 글자가 5개면, 나머지 95개 프레임은 뭘 출력해요? Blank를 출력하면 돼요.
>
> **둘째**, 같은 글자 연속을 구분해요. 영어 "book"의 "oo"처럼요. 사이에 Blank가 있으면 별개로 인식해요.

---

## 슬라이드 18: CTC 디코딩 과정

**[시각자료: 05_ctc_decoding_process.png]**

> 이 그림이 CTC 디코딩 과정이에요. 세 단계!
>
> **Step 0**: 모델이 각 프레임마다 확률 높은 글자를 출력해요.
> 결과: "ε-H-H-ε-E-ε-L-L-L-ε-O"
>
> **Step 1**: 연속 중복 제거. H-H → H, L-L-L → L
> 결과: "ε-H-ε-E-ε-L-ε-O"
>
> **Step 2**: Blank 제거.
> 최종 결과: **"HELO"**
>
> 간단하죠? 정확한 정렬을 몰라도 결과를 얻을 수 있어요.

---

## 슬라이드 19: CTC 퀴즈

**[시각자료: 퀴즈 슬라이드]**

> 퀴즈 하나!
>
> CTC 출력이 "ε-C-C-ε-A-A-ε-T-ε"라면, 최종 결과는?
>
> (3초 대기)
>
> 네, **"CAT"**입니다!
>
> 하나 더! "C-ε-C-A-T"는요?
>
> (3초 대기)
>
> 이건 **"CCAT"**이에요! 첫 번째 C랑 세 번째 C 사이에 Blank가 있어서 별개로 취급해요. 중복 제거 안 해요.
>
> 이게 Blank의 두 번째 역할이었죠!

---

## 슬라이드 20: CTC의 한계

**[시각자료: CTC 한계]**

> 근데 CTC도 한계가 있어요.
>
> CTC는 각 프레임을 **독립적으로** 예측해요. 앞뒤 맥락을 안 봐요.
>
> 예: "I read a book" vs "I read yesterday"
>
> "read" 발음이 다르잖아요. 리드 vs 레드. 맥락을 봐야 아는데, CTC는 이게 어려워요.
>
> 그래서 나온 게 **Attention**!

---

## 슬라이드 21: Attention 메커니즘

**[시각자료: 06_attention_heatmap.png]**

> **Attention**은 "입력의 어디에 집중할지" 학습하는 거예요.
>
> 이 히트맵 보세요. X축이 입력 프레임, Y축이 출력 글자.
>
> 대각선으로 밝은 부분이 있죠? "an"을 출력할 때 앞쪽 프레임에 집중하고, "yo"를 출력할 때 뒤쪽 프레임에 집중해요.
>
> **어떤 입력을 참고해서 출력할지** 모델이 스스로 학습해요.
>
> CTC와 달리, Attention은 **이전 출력도 참고**해요. 그래서 맥락 파악이 더 잘 돼요.

---

## 슬라이드 22: Transformer

**[시각자료: Transformer 다이어그램]**

> 2017년에 구글이 **Transformer**를 발표했어요. "Attention Is All You Need"라는 유명한 논문이죠.
>
> Transformer는 Attention만으로 모든 걸 처리해요. 이전에 쓰던 RNN 없이도요.
>
> 장점? **병렬 처리**가 돼요. RNN은 순차적으로 해야 하는데, Transformer는 한 번에 처리해요. 학습이 훨씬 빨라졌어요.
>
> 요즘 최신 모델 **Whisper** 같은 것도 다 Transformer 기반이에요.
>
> [🔧 개발자 포인트] OpenAI Whisper는 오픈소스예요. 로컬에서 돌릴 수 있어요. Python으로 `pip install openai-whisper` 하면 바로 써볼 수 있어요.

---

## 슬라이드 23: Beam Search

**[시각자료: 07_beam_search_tree.png]**

> 마지막으로 **디코딩 전략**!
>
> 모델이 매 순간 여러 후보를 제안해요. 뭘 선택하죠?
>
> **Greedy**: 매 순간 가장 확률 높은 거 선택. 빠르지만 전체 최적이 아닐 수도 있어요.
>
> **Beam Search**: 상위 k개 후보를 유지하면서 탐색. 이 그림이 k=3인 경우예요.
>
> 보시면 H, A, T 세 후보 유지 → HE, AN, AT 확장 → 또 상위 3개만... 이렇게 해요.
>
> Beam Search가 더 정확하지만 느려요. 상황에 따라 선택하면 됩니다.

---

## 슬라이드 24: Chapter 2 정리

**[시각자료: 요약 박스]**

> 챕터 2 정리!
>
> 1. STT 핵심 난제: **Alignment 문제**
> 2. **CTC**: Blank + 중복 제거로 해결
> 3. **Attention**: 어디에 집중할지 학습
> 4. **Transformer**: Attention만으로 빠르게
> 5. **Beam Search**: 여러 후보 중 최적 선택
>
> 자, 이제 10분 쉬겠습니다! 🎉
>
> 화장실 다녀오시고, 커피 한 잔 하시고 오세요.

---

# ☕ 휴식 (10분)

---

# 📘 Chapter 3: TTS - 텍스트를 소리로 (25분)

## 슬라이드 25: 챕터 3 타이틀

**[시각자료: Chapter 3 타이틀]**

> 다들 오셨나요? 세 번째 챕터입니다.
>
> **TTS(Text-to-Speech)**, 텍스트를 음성으로 바꾸는 거예요.
>
> STT가 소리 → 텍스트였다면, TTS는 반대로 **텍스트 → 소리**예요.
>
> 근데 단순히 글자를 읽는 게 아니에요. 자연스러운 억양, 리듬, 감정까지 담아야 해요. 로봇 같으면 안 되잖아요.

---

## 슬라이드 26: TTS 2단계 구조

**[시각자료: 08_tts_pipeline.png]**

> 현대 TTS는 보통 **2단계**로 나뉩니다.
>
> **Stage 1: Acoustic Model** - 텍스트 → Mel-Spectrogram
> 어떤 발음으로, 어떤 억양으로, 얼마나 길게 말할지 결정해요.
> Tacotron2, FastSpeech2 같은 모델이에요.
>
> **Stage 2: Vocoder** - Mel-Spectrogram → 실제 오디오
> 스펙트로그램을 진짜 소리 파형으로 바꿔요.
> HiFi-GAN이 대표적이에요.
>
> 왜 나누냐? 각각 따로 발전시킬 수 있거든요. 더 좋은 Vocoder 나오면 갈아끼우면 돼요.

---

## 슬라이드 27: Text Normalization

**[시각자료: 정규화 예시]**

> Acoustic Model에 넣기 전에 먼저 **텍스트 정규화**가 필요해요.
>
> 예를 들어 "123"을 어떻게 읽어요?
> - "일이삼"? "백이십삼"?
>
> "2024년 12월 20일"은?
> - "이천이십사년 십이월 이십일"
>
> "Dr. Kim"은?
> - "닥터 김"? "의사 김"?
>
> 이런 것들을 **발음 가능한 형태로 바꾸는 게** 텍스트 정규화예요.
>
> 규칙이 은근히 많고 복잡해요. 한국어는 조사도 앞 글자에 따라 바뀌잖아요. "을/를", "이/가".

---

## 슬라이드 28: G2P 변환

**[시각자료: 09_g2p_examples.png]**

> 다음으로 **G2P(Grapheme to Phoneme)**, 문자를 발음으로 바꾸는 거예요.
>
> 한국어는 **"적힌 대로 안 읽는"** 경우가 많아요.
>
> 그림 보세요.
> - "같이(gat-i)" → "가치(ga-chi)" - **구개음화**
> - "학교(hak-gyo)" → "학꾜(hak-ggyo)" - **경음화**
> - "국민(guk-min)" → "궁민(gung-min)" - **비음화**
> - "음악을(eum-ak-eul)" → "으마글(eu-ma-geul)" - **연음**
> - "좋아요(joh-a-yo)" → "조아요(jo-a-yo)" - **ㅎ탈락**
>
> 이런 음운 규칙 다 적용해야 자연스럽게 들려요.
>
> [🔧 개발자 포인트] 한국어 G2P 직접 구현하기 어려워요. `g2pk` 같은 라이브러리 쓰거나, API 쓰는 게 현실적이에요.

---

## 슬라이드 29: Vocoder 발전

**[시각자료: Vocoder 비교]**

> 이제 2단계, **Vocoder**!
>
> 예전에는 **Griffin-Lim**이라는 알고리즘을 썼어요. 수학적으로 역변환하는 건데... 솔직히 로봇 같아요. "징글징글"한 느낌.
>
> 요즘은 **Neural Vocoder**, 딥러닝 기반이에요.
>
> **HiFi-GAN**이 대표적인데, 사람 목소리랑 거의 구분 안 돼요. 실시간 생성도 되고요.
>
> [🔧 개발자 포인트] HiFi-GAN 오픈소스 있어요. GPU 있으면 로컬에서 돌릴 수 있어요.

---

## 슬라이드 30: GAN 구조

**[시각자료: GAN 다이어그램]**

> HiFi-GAN의 **GAN**이 뭐냐?
>
> **GAN(Generative Adversarial Network)**, 두 네트워크가 경쟁하면서 학습해요.
>
> **Generator(생성자)**: Mel-Spec 받아서 오디오 만들어요.
>
> **Discriminator(판별자)**: 진짜인지 가짜인지 판별해요.
>
> Generator는 Discriminator를 속이려 하고, Discriminator는 안 속으려 해요.
>
> 이렇게 경쟁하다 보면 Generator가 **진짜와 구분 안 되는 가짜**를 만들어내요.
>
> 비유하자면, 위조지폐범(Generator)과 경찰(Discriminator)이 계속 경쟁하면서 위조 기술이 점점 정교해지는 거예요.

---

## 슬라이드 31: End-to-End TTS

**[시각자료: VITS 다이어그램]**

> 요즘은 **End-to-End TTS**도 있어요.
>
> **VITS** 같은 모델은 2단계를 하나로 합쳤어요. 텍스트 → 바로 오디오.
>
> 장점: 더 자연스럽고, 파이프라인이 단순해요.
>
> 단점: 학습이 어렵고, 각 단계를 따로 최적화하기 어려워요.
>
> 상황에 따라 선택하면 됩니다.

---

## 슬라이드 32: Chapter 3 정리

**[시각자료: 요약 박스]**

> 챕터 3 정리!
>
> 1. TTS = **2단계**: Acoustic Model → Vocoder
> 2. **Text Normalization**: 숫자, 기호 등 정규화
> 3. **G2P**: 문자 → 발음 (음운 규칙 중요!)
> 4. **Vocoder**: HiFi-GAN이 현재 표준
> 5. **GAN**: Generator vs Discriminator 경쟁
> 6. **End-to-End**(VITS)도 부상 중
>
> 다음은 마지막 챕터, 실무 이야기입니다!

---

# 📘 Chapter 4: 평가지표와 실무 이슈 (20분)

## 슬라이드 33: 챕터 4 타이틀

**[시각자료: Chapter 4 타이틀]**

> 네 번째 챕터, **"개발자가 알아야 할 평가지표와 실무 이슈"**.
>
> 이론은 알겠는데, 실제로 쓸 때 뭘 봐야 할까요?
>
> STT 얼마나 잘 맞추는지, TTS 얼마나 자연스러운지 어떻게 측정해요?
>
> 실무에서 주의할 점은 뭘까요?

---

## 슬라이드 34: WER (Word Error Rate)

**[시각자료: 10_wer_cer_example.png 상단]**

> STT 평가의 대표 지표, **WER(Word Error Rate)**.
>
> 공식: **(S + D + I) / N**
> - S: 대체(다른 단어로 바뀜)
> - D: 삭제(빠짐)
> - I: 삽입(없는 게 추가됨)
> - N: 정답 단어 수
>
> 그림 보세요.
> 정답: "the weather is good"
> 예측: "the weather is great"
>
> "good"이 "great"으로 바뀜 → S=1
>
> WER = 1/4 = **25%**
>
> 낮을수록 좋은 거예요.

---

## 슬라이드 35: CER (Character Error Rate)

**[시각자료: 10_wer_cer_example.png 하단]**

> 근데 **한국어는 WER보다 CER**이 더 적합해요.
>
> 왜? 한국어는 교착어거든요. 조사 하나만 틀려도 단어 전체가 오류로 처리돼요.
>
> 예: "학교에" vs "학교가"
> - WER: 단어가 틀림 → 100% 오류
> - CER: 7글자 중 1글자 틀림 → 약 14%
>
> CER이 더 **공정한 평가**예요.
>
> [🔧 개발자 포인트] 한국어 STT 평가할 때 WER만 보지 마세요. CER도 같이 보시고, 실제 사용자 경험도 중요해요.

---

## 슬라이드 36: MOS (Mean Opinion Score)

**[시각자료: MOS 점수표]**

> TTS 평가는 **MOS(Mean Opinion Score)**.
>
> 사람들한테 들려주고 1~5점으로 평가받는 거예요.
>
> - 5점: 매우 자연스럽다
> - 4점: 자연스럽다
> - 3점: 보통
> - 2점: 부자연스럽다
> - 1점: 매우 부자연스럽다
>
> 최신 TTS 모델들은 **4.0점 이상**이 많아요.
>
> 참고로 진짜 사람 목소리가 보통 4.5점 정도. 거의 근접한 거죠.

---

## 슬라이드 37: Latency

**[시각자료: Latency 구성도]**

> 실무에서 중요한 게 **Latency(지연 시간)**.
>
> 음성 비서나 실시간 자막은 빨라야 하잖아요?
>
> 지연 요소들:
> - 오디오 전송
> - 전처리
> - 모델 추론 ← 여기가 제일 오래 걸려요
> - 후처리
> - 결과 전송
>
> 핵심은 **스트리밍**이에요. 전체 다 받고 처리하면 너무 느려요. 조금씩 들어오는 대로 처리해야 해요.

---

## 슬라이드 38: 노이즈 대응

**[시각자료: 노이즈 유형표]**

> 실제 환경에는 **노이즈**가 있어요.
>
> - **백색 소음**(에어컨, 팬): Spectral Subtraction
> - **배경 음악**: 음악/음성 분리 모델
> - **다중 화자**: Speaker Diarization
> - **반향/에코**: Dereverberation
>
> 가장 효과적인 건 **Data Augmentation**!
> 노이즈 섞인 데이터로 학습하면 모델이 노이즈에 강해져요.

---

## 슬라이드 39: 실무 체크리스트

**[시각자료: 체크리스트]**

> 실무 체크리스트!
>
> **STT**:
> - 샘플링 레이트 16kHz 확인
> - 오디오 포맷 통일
> - 묵음 구간 처리 정책
> - 실시간 vs 배치 결정
> - 개인정보 정책 (음성은 민감 데이터!)
>
> **TTS**:
> - 텍스트 정규화 규칙
> - G2P 커버리지 테스트
> - 출력 포맷 결정
> - 스트리밍 지원 여부
> - 저작권 확인

---

## 슬라이드 40: Chapter 4 정리

**[시각자료: 요약 박스]**

> 챕터 4 정리!
>
> 1. **STT 평가**: WER(영어), CER(한국어 권장)
> 2. **TTS 평가**: MOS (4.0+ 이면 사람에 근접)
> 3. **Latency**: 스트리밍 필수
> 4. **노이즈**: 전처리 + Data Augmentation
> 5. **체크리스트** 꼭 확인!
>
> 자, 이론은 여기까지입니다! 🎉
>
> 이제 직접 체험해볼 시간이에요!

---

# 🛠️ 실습 환경 안내 (5분)

## 슬라이드 41: 실습 안내

**[시각자료: 실습 안내 슬라이드]**

> 이제 실습 시간입니다!
>
> 제가 준비한 웹 앱 **SoundLab**으로 직접 체험해볼 거예요.
>
> **준비물**:
> - Chrome 또는 Edge 브라우저 (Firefox는 음성인식 안 돼요!)
> - 마이크 (노트북 내장도 OK)
> - 인터넷 연결
>
> 지금 채팅창에 URL 공유할게요.
>
> **https://soundlab-demo.vercel.app** (예시)
>
> 접속하시고, 마이크 권한 허용해주세요.
>
> 다들 들어오셨나요? 화면에 뭐가 보이세요?
>
> (참석자 확인 대기)
>
> 좋습니다! 그럼 시작해볼게요.

---

# 🧪 실습 1: Wave Lab - 소리 시각화 (15분)

## 슬라이드 42: Wave Lab 소개

**[시각자료: Wave Lab 스크린샷]**

> 첫 번째 실습, **Wave Lab**!
>
> 아까 이론에서 배운 거 기억나시죠? 소리 → 파형 → 스펙트로그램.
>
> 지금 직접 눈으로 볼 거예요.

---

## 실습 1-1: 실시간 파형 보기

> **[녹음 버튼]** 눌러보세요.
>
> 마이크 권한 허용하시고요.
>
> 이제 뭔가 말해보세요. "안녕하세요~"
>
> 화면에 파형이 실시간으로 움직이죠? 이게 **시간 도메인** 표현이에요.
>
> 크게 말하면 파형이 커지고, 조용히 하면 작아지죠? 그게 **진폭**이에요.
>
> (30초 체험)

---

## 실습 1-2: FFT 주파수 분석

> 이제 **FFT 탭**으로 가볼게요.
>
> 다시 녹음하면서, "아~" 하고 길게 소리내보세요.
>
> 막대그래프가 보이시죠? X축이 주파수예요.
>
> 낮은 소리 "우~" 하면 왼쪽이 올라가고,
> 높은 소리 "이~" 하면 오른쪽이 올라가요.
>
> 직접 해보세요! "우~" 그리고 "이~"
>
> (30초 체험)
>
> 어때요? 차이 보이시죠?

---

## 실습 1-3: 스펙트로그램

> 이제 **스펙트로그램 탭**!
>
> 녹음하고 "안녕하세요"라고 말해보세요.
>
> (녹음 완료 후)
>
> 보시면 가로가 시간, 세로가 주파수, 색깔이 세기예요.
>
> 말하는 부분은 밝고, 조용한 부분은 어둡죠?
>
> 이게 바로 **STT 모델이 보는 입력**이에요. 이 그림 보고 "안녕하세요"를 맞추는 거죠.
>
> 신기하죠?

---

## 실습 1-4: 샘플링 레이트 실험

> 마지막으로 **샘플링 데모**!
>
> 여기서 샘플링 레이트를 바꿔볼 수 있어요.
>
> 44,100Hz로 녹음한 거랑 8,000Hz로 다운샘플링한 거 비교해보세요.
>
> **[재생]** 눌러보시면...
>
> 8,000Hz는 전화 통화 품질이에요. 좀 뭉개지죠?
>
> 이게 나이퀴스트 정리랑 연결돼요. 샘플링 낮으면 고주파가 날아가요.
>
> 음성 인식용 16,000Hz는 목소리 인식에는 충분하지만, 음악 감상용은 아니에요.

---

# 🧪 실습 2: STT Lab - 음성 인식 체험 (20분)

## 슬라이드 43: STT Lab 소개

**[시각자료: STT Lab 스크린샷]**

> 두 번째 실습, **STT Lab**!
>
> 이론에서 배운 CTC, Attention 기억나시죠?
>
> 직접 시뮬레이션해보고, 실제 음성 인식도 해볼 거예요.

---

## 실습 2-1: CTC 시뮬레이터

> **CTC 시뮬레이터** 탭으로 가세요.
>
> 여기에 CTC 출력을 직접 입력할 수 있어요.
>
> 한번 이거 넣어볼게요: `ε-H-H-ε-E-ε-L-L-L-ε-O`
>
> **[디코딩]** 버튼 누르면?
>
> Step 1: 중복 제거 → ε-H-ε-E-ε-L-ε-O
> Step 2: Blank 제거 → **HELO**
>
> 이게 아까 그림에서 본 과정이에요!
>
> 이번엔 직접 해보세요. `C-ε-C-A-A-T` 넣으면 뭐가 나올까요?
>
> (20초 대기)
>
> **CCAT**! 맞추셨나요?

---

## 실습 2-2: 실시간 음성 인식

> 이제 진짜 음성 인식!
>
> **Speech Recognition** 탭 가세요.
>
> **[시작]** 버튼 누르고, 말해보세요.
>
> "오늘 날씨가 좋네요."
>
> (인식 결과 확인)
>
> 어때요? 잘 맞추죠?
>
> 이게 브라우저에 내장된 **Web Speech API**예요. 실제로 Google 음성 인식 엔진이 돌아가요.
>
> 한번 더 해볼게요. 이번엔 일부러 어렵게!
>
> "콜라 한 잔 주세요" vs "콜라 한잔주세요"
>
> 띄어쓰기에 따라 결과가 달라질 수도 있어요.
>
> 또는 방언이나 억양 다르게 해보세요. 어떻게 되는지.
>
> (자유 실험 1분)

---

## 실습 2-3: Confidence Score

> 인식 결과 옆에 **신뢰도(Confidence)** 점수 보이시죠?
>
> 0.9 이상이면 꽤 확신하는 거고, 0.7 이하면 불확실한 거예요.
>
> 일부러 작게 말하거나, 노이즈 섞어보세요. 신뢰도가 떨어질 거예요.
>
> 실무에서는 이 점수 보고 **"잘 못 들었어요, 다시 말해주세요"** 할지 판단해요.
>
> [🔧 개발자 포인트] API 응답에 confidence 값 항상 체크하세요. 낮으면 재입력 요청하는 로직 넣으면 좋아요.

---

# 🧪 실습 3: TTS Lab - 음성 합성 체험 (15분)

## 슬라이드 44: TTS Lab 소개

**[시각자료: TTS Lab 스크린샷]**

> 세 번째 실습, **TTS Lab**!
>
> 텍스트를 음성으로 바꿔볼 거예요.

---

## 실습 3-1: G2P 변환 체험

> **G2P 변환기** 탭으로 가세요.
>
> 여기에 "같이"라고 입력해보세요.
>
> 결과가 "가치"로 나오죠? **구개음화**!
>
> 다른 것도 해볼게요.
> - "학교" → "학꾜" (경음화)
> - "국민" → "궁민" (비음화)
>
> 이런 음운 규칙을 TTS가 다 알아야 자연스럽게 읽을 수 있어요.
>
> 직접 단어 몇 개 넣어보세요!

---

## 실습 3-2: 실시간 TTS

> **Speech Synthesis** 탭!
>
> 텍스트 입력하고 **[읽기]** 버튼 누르면 소리가 나요.
>
> "안녕하세요, 반갑습니다."
>
> (재생)
>
> 어때요? 꽤 자연스럽죠?
>
> 이것도 브라우저 내장 **Web Speech API**예요.
>
> **속도(Rate)**랑 **음높이(Pitch)** 슬라이더 조절해보세요.
>
> 빠르게, 느리게, 높게, 낮게...
>
> (30초 실험)

---

## 실습 3-3: 음성 선택

> **Voice 선택** 드롭다운 보시면 여러 목소리가 있어요.
>
> 한국어 목소리, 영어 목소리 다 있어요.
>
> 다른 목소리로 바꿔서 들어보세요.
>
> 같은 문장인데 목소리에 따라 느낌이 다르죠?
>
> 실제 서비스 만들 때 **어떤 목소리 쓸지**도 중요한 결정이에요. 브랜드 이미지랑도 연결되거든요.

---

## 실습 3-4: STT → TTS 순환

> 마지막으로 **Playground** 탭!
>
> 여기서 **STT → TTS 순환** 테스트를 할 수 있어요.
>
> 내가 말하면 → 텍스트로 인식 → 다시 음성으로 읽어줌
>
> 원본 내 목소리랑 TTS 목소리 비교해보세요.
>
> **[시작]** 누르고 말해보세요.
>
> "오늘 회의는 3시에 시작합니다."
>
> (순환 테스트)
>
> 어때요? STT가 잘못 인식하면 TTS도 이상하게 읽겠죠?
>
> **"전화"를 "전하"로 인식했다**면, TTS도 "전하"라고 읽을 거예요.
>
> 이게 실제 서비스에서 **오류 전파** 문제예요. 앞단에서 틀리면 뒷단도 틀려요.

---

# 🎬 마무리 & Q&A (10분)

## 슬라이드 45: 전체 정리

**[시각자료: 전체 흐름도]**

> 자, 3시간 동안 수고 많으셨습니다!
>
> 오늘 배운 걸 한 장으로 정리하면:
>
> **소리** → (샘플링, 양자화) → **디지털 오디오** → (STFT) → **Mel-Spectrogram**
>
> **STT**: Mel-Spec → (CTC/Attention/Transformer) → **텍스트**
>
> **TTS**: 텍스트 → (정규화, G2P) → Acoustic Model → Vocoder → **오디오**
>
> 이 흐름만 기억하시면 STT/TTS 원리는 이해하신 거예요!

---

## 슬라이드 46: 핵심 메시지

**[시각자료: 핵심 메시지]**

> 오늘 가져가셨으면 하는 핵심 메시지:
>
> 1. **STT/TTS는 블랙박스가 아니다** - 각 단계에서 뭘 하는지 알면 문제 해결이 쉬워요
>
> 2. **평가 지표를 알아야 한다** - WER/CER, MOS. 숫자로 비교할 수 있어야 해요
>
> 3. **한국어는 특별하다** - G2P 음운 규칙, CER 평가 등 한국어만의 특성이 있어요
>
> 4. **직접 체험이 최고** - 오늘 SoundLab으로 해본 거, 기억에 남으시죠?

---

## 슬라이드 47: 참고 자료

**[시각자료: 참고 자료 목록]**

> 더 공부하고 싶으신 분들:
>
> **논문**:
> - Attention Is All You Need (Transformer)
> - Whisper (OpenAI)
> - HiFi-GAN
>
> **오픈소스**:
> - OpenAI Whisper: `pip install openai-whisper`
> - Coqui TTS: `pip install TTS`
>
> **학습 자료**:
> - Hugging Face Audio Course (무료)
>
> 발표 자료는 따로 공유해드릴게요.

---

## 슬라이드 48: Q&A

**[시각자료: Q&A 슬라이드]**

> 자, 이제 Q&A 시간입니다!
>
> 이론이든 실습이든, 실무 경험이든 뭐든 질문해주세요.
>
> ...
>
> (질문 답변)
>
> ...

---

## 슬라이드 49: Thank You

**[시각자료: 마무리 슬라이드]**

> 3시간 동안 집중해주셔서 정말 감사합니다!
>
> 오늘 배운 내용이 실무에서 도움이 되셨으면 좋겠어요.
>
> 추가 질문 있으시면 [이메일/슬랙]으로 연락주세요.
>
> 다들 수고하셨습니다! 👏

---

# 📎 부록: 예상 Q&A

## Q1: Whisper vs 클라우드 API?

> **A**: 상황에 따라요.
> 
> - Whisper: 무료, 로컬 처리 가능 (개인정보 안전), GPU 필요
> - 클라우드 API: 바로 사용 가능, 비용 발생, 데이터 외부 전송
> 
> 민감한 데이터 → Whisper
> 빠른 개발 → API

## Q2: 한국어 TTS 오픈소스?

> **A**: Coqui TTS가 괜찮아요. 다만 품질은 상용 서비스(네이버, 구글)가 더 좋아요.

## Q3: 실시간 STT 핵심?

> **A**: **청크 단위 스트리밍 처리**. 전체 다 받고 하면 느려요. 0.5~1초씩 끊어서 처리해야 해요.

## Q4: Web Speech API 상용 서비스에 써도 되나요?

> **A**: 주의 필요해요. 브라우저/OS마다 다르고, 인터넷 필요하고, SLA 없어요. 프로토타입은 OK, 프로덕션은 전용 API 권장.

## Q5: STT 정확도 높이려면?

> **A**: 
> 1. 마이크 품질 개선
> 2. 노이즈 전처리
> 3. 도메인 특화 모델 (의료, 법률 등)
> 4. 후처리 (맞춤법 교정, 특수 용어 사전)

---

> **스크립트 버전**: 2.0 (이론 + 실습 완전판)
> 
> **총 시간**: 3시간
> - 이론: 2시간 (슬라이드 1-40)
> - 실습: 1시간 (슬라이드 41-49)
> 
> **슬라이드 수**: 49장
